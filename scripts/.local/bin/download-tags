#!/usr/bin/env python3

from bs4 import BeautifulSoup as bsoup
from dateutil.parser import parse as date_parse
import base64
import os
import requests
import stat
import sys
import toml

file_name = None

def parse_file(f):
    files = []
    count = 1
    for line in f.readlines():
        if len(line.strip()) == 0:
            continue
        try:
            file,link,link_type = line.split(',')
        except:
            print(f'Failed on line {count} of file {os.getcwd()}/{f.name}')
            exit(1)

        files.append((file,link.strip(),link_type.strip()))
        count += 1
    return files

def download_list_item_metadata(file, link, link_type):
    is_folder = stat.S_ISDIR(os.stat(file).st_mode)
    if is_folder:
        download_folder_metadata(file, link, link_type)
    else:
        download_file_metadata(file, link, link_type)


def download_folder_metadata(folder, link, link_type):
    global file_name
    cwd = os.getcwd()
    os.chdir(folder)
    print(f'\033[34mDownloading folder {folder}\033[00m')

    print(f'\033[31mNot downloading folder metadata for {folder}:{link}\033[00m')

    with open(file_name, 'r') as f:
        files = parse_file(f)

    for file, link, link_type in files:
        download_file_metadata(file, link, link_type)

    os.chdir(cwd)

def download_file_metadata(file, link, link_type):
    info_file = f'{file}.toml'
    try:
        os.stat(info_file)
        print(f'\033[34mMetadata already exists for {file}. Skipping!\033[00m')
        return
    except FileNotFoundError:
        pass

    match link_type:
        case 'deviantart':
            request = requests.get(link)
            soup = bsoup(request.content, 'lxml')
            try:
                content = parse_deviantart(soup)
            except Exception as e:
                print(f'\033[31mFailed on {file}: {e}\nLink: {link}\033[00m')
                return

            if len(content['tags']) == 0:
                print(f'\033[31mNo tags found for {file}!\033[00m')

            content['type'] = 'deviantart'
            content['file-name'] = file
            content['link'] = link
            with open(info_file, 'w') as f:
                toml.dump(content, f)
                print(f'\033[32mWrote metadata for {file}\033[00m')
        case other:
            print(f'Unknown link type {link_type}')

def parse_deviantart_tags(soup):
    div = soup.find('div', {'class': '_2n9lY'})
    tags = div.find_all('a')
    tags = list(map(lambda x: x['href'].split('/')[-1], div.find_all('a')))
    return tags

def parse_deviantart(soup):
    content = {}
    try:
        content['title'] = soup.find('h1', {'data-hook': 'deviation_title'}).text
    except:
        raise Exception('Failed on title')

    try:
        content['author'] = soup.find('a', {'data-hook': 'user_link'})['data-username']
    except:
        raise Exception('Failed on author')

    try:
        avatar_link = soup.find('a', {'data-hook': 'user_link'})['data-icon']
        img = requests.get(avatar_link).content
        content['author-avatar'] = base64.b64encode(img).decode('utf-8')
    except:
        raise Exception('Failed on author avatar')

    try:
        content['tags'] = parse_deviantart_tags(soup)
    except:
        content['tags'] = []

    try:
        content['time'] = date_parse(soup.find('time')['datetime'])
    except:
        raise Exception('Failed on time')

    return content


if __name__ == '__main__':
    try:
        file_name = sys.argv[1]
    except:
        print('First argument must be the file name in the format `file_name,link,link_type`!')
        exit(1)

    dirname = os.path.dirname(file_name)
    if len(dirname.strip()):
        print(f'Cding into {dirname}')
        os.chdir(dirname)

    file_name = os.path.basename(file_name)
    with open(file_name, 'r') as f:
        files = parse_file(f)

    for file,link,link_type in files:
        download_list_item_metadata(file, link, link_type)

    print('Done!')
